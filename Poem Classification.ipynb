{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "\n",
    "poetry = pd.read_csv('all.csv')\n",
    "num_poems = 573\n",
    "\n",
    "train=poetry.sample(frac=0.8726,random_state=200)\n",
    "test=poetry.drop(train.index)\n",
    "\n",
    "poem = train.iloc[:,1]\n",
    "poems = []\n",
    "for item in poem:\n",
    "    poems.append(item)\n",
    "    \n",
    "y = train.iloc[:, 4]\n",
    "one_hot = pd.get_dummies(y)\n",
    "labels = one_hot.values\n",
    "\n",
    "poem1 = test.iloc[:,1]\n",
    "poems1 = []\n",
    "for item in poem1:\n",
    "    poems1.append(item)\n",
    "    \n",
    "y1 = test.iloc[:, 4]\n",
    "one_hot = pd.get_dummies(y1)\n",
    "labels1 = one_hot.values\n",
    "\n",
    "poem_train = poems\n",
    "poem_test = poems1\n",
    "\n",
    "train_y = labels\n",
    "test_y = labels1\n",
    "\n",
    "#Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# We need to import several things from Keras.\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU,LSTM, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Tokenization\n",
    "num_words = 2000 # setting the max number of words = vocabulary\n",
    "tokenizer = Tokenizer(num_words = num_words) # creating the tokenizer\n",
    "data_text = poem_train + poem_test # creating our tokenizer with the entire dataset\n",
    "\n",
    "tokenizer.fit_on_texts(data_text) # fitting the tokenizer to data\n",
    "train_x_tokens = tokenizer.texts_to_sequences(poem_train)\n",
    "test_x_tokens = tokenizer.texts_to_sequences(poem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    }
   ],
   "source": [
    "num_tokens = [len(tokens) for tokens in train_x_tokens + test_x_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "# The max number of tokens is set to the average plus 2 standard deviations\n",
    "max_tokens = np.mean(num_tokens)+ 2 * np.std(num_tokens)\n",
    "\n",
    "#Converting the value to int\n",
    "max_tokens = int(max_tokens)\n",
    "print(max_tokens)\n",
    "\n",
    "pad = 'pre' # add 0's to the first places of the array\n",
    "train_x_pad = pad_sequences(train_x_tokens, maxlen = max_tokens, padding= pad, truncating = pad)\n",
    "test_x_pad = pad_sequences(test_x_tokens, maxlen=max_tokens , padding = pad, truncating = pad)\n",
    "\n",
    "# Tokenizer Inverse Map\n",
    "#  a function that will translate the integer-tokens back to words\n",
    "\n",
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "\n",
    "# Helper function for converting a list of tokends back to a string of words\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to the words:\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    #Concatenate all the words\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 700, 8)            16000     \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 700, 25)           3400      \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 15)                2460      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 48        \n",
      "=================================================================\n",
      "Total params: 21,908\n",
      "Trainable params: 21,908\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_size = 8\n",
    "model.add(Embedding(input_dim = num_words, output_dim = embedding_size\n",
    "                    , input_length = max_tokens, name = 'layer_embedding'))\n",
    "model.add(LSTM(25, return_sequences = True))\n",
    "model.add(LSTM(15))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "optimizer = Adam(lr=1e-2)\n",
    "model.compile(loss ='categorical_crossentropy', optimizer =  optimizer, metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1/8\n",
      "475/475 [==============================] - 13s 28ms/step - loss: 0.9537 - acc: 0.5558 - val_loss: 0.7655 - val_acc: 0.6800\n",
      "Epoch 2/8\n",
      "475/475 [==============================] - 6s 14ms/step - loss: 0.9410 - acc: 0.5537 - val_loss: 0.7919 - val_acc: 0.6800\n",
      "Epoch 3/8\n",
      "475/475 [==============================] - 6s 13ms/step - loss: 0.9287 - acc: 0.5537 - val_loss: 0.8088 - val_acc: 0.6800\n",
      "Epoch 4/8\n",
      "475/475 [==============================] - 6s 13ms/step - loss: 0.9136 - acc: 0.5558 - val_loss: 0.7952 - val_acc: 0.7200\n",
      "Epoch 5/8\n",
      "475/475 [==============================] - 6s 12ms/step - loss: 0.8210 - acc: 0.6695 - val_loss: 0.7678 - val_acc: 0.7200\n",
      "Epoch 6/8\n",
      "475/475 [==============================] - 6s 13ms/step - loss: 0.7248 - acc: 0.7179 - val_loss: 0.8134 - val_acc: 0.7200\n",
      "Epoch 7/8\n",
      "475/475 [==============================] - 6s 12ms/step - loss: 0.6778 - acc: 0.7347 - val_loss: 0.8752 - val_acc: 0.6800\n",
      "Epoch 8/8\n",
      "475/475 [==============================] - 6s 12ms/step - loss: 0.5659 - acc: 0.7874 - val_loss: 0.9949 - val_acc: 0.6400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x2324d8df828>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x_pad, train_y, validation_split = 0.05, epochs = 8, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 4ms/step\n",
      "Accuracy : 68.49%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_x_pad, test_y)\n",
    "print(\"Accuracy : {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
